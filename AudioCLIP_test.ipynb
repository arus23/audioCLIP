{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d63a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install ftfy\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import simplejpeg\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision as tv\n",
    "\n",
    "import pyaudio\n",
    "import IPython.display as ipd\n",
    "import wave\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "sys.path.append(os.path.abspath(f'{os.getcwd()}/..'))\n",
    "\n",
    "from model import AudioCLIP\n",
    "from utils.transforms import ToTensor1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70dbdf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "MODEL_FILENAME = 'AudioCLIP-Full-Training.pt'\n",
    "\n",
    "# derived from ESResNeXt\n",
    "SAMPLE_RATE = 44100\n",
    "\n",
    "# derived from CLIP\n",
    "IMAGE_SIZE = 224\n",
    "IMAGE_MEAN = 0.48145466, 0.4578275, 0.40821073\n",
    "IMAGE_STD = 0.26862954, 0.26130258, 0.27577711\n",
    "\n",
    "LABELS = ['cat', 'thunderstorm', 'coughing', 'alarm clock', 'car horn', 'door wood knock', 'mouse click', 'keyboard typing', 'sneezing', 'laughing', 'dog', 'rain']\n",
    "\n",
    "# Model Instantiation\n",
    "\n",
    "aclp = AudioCLIP(pretrained=f'./assets/{MODEL_FILENAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f3be5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ec71fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RATE = 44100\n",
    "CHANNELS = 1\n",
    "RECORD_SECONDS = 12\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "LENGTH = 220500\n",
    "FILENAME = \"door_knock\"\n",
    "# CHUNK = int(RATE/20) # RATE / number of updates per second\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "stream = p.open(format = FORMAT, \n",
    "                channels = CHANNELS, \n",
    "                rate = RATE, \n",
    "                input = True,\n",
    "                frames_per_buffer = CHUNK)\n",
    "\n",
    "frames = []\n",
    "\n",
    "for _ in range (0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "\n",
    "        # Read audio data from the stream\n",
    "        raw_data = stream.read(CHUNK)\n",
    "\n",
    "        frames.append(raw_data)\n",
    "\n",
    "        # Convert the raw data to a NumPy array\n",
    "        # data = np.frombuffer(raw_data, dtype=np.int16).astype(np.float32)\n",
    "\n",
    "        # Your processing or analysis code goes here\n",
    "        # For example, print the length of the audio data\n",
    "        # print(f\"Received {len(data)} frames of audio data.\")\n",
    "\n",
    "# Close the audio stream and terminate PyAudio\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "\n",
    "data = np.frombuffer(b''.join(frames), dtype=np.int16)\n",
    "\n",
    "# files = f\"./test_audio/{FILENAME}.wav\"\n",
    "# wf = wave.open(files, \"wb\")\n",
    "# wf.setnchannels(CHANNELS)\n",
    "# wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "# wf.setframerate(RATE)\n",
    "# wf.writeframes(data)\n",
    "# wf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30c68be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if data.dtype in (np.int8, np.uint8, np.int16, np.int32, np.int64):\n",
    "        data_x = data.astype(np.float32) / (np.iinfo(data.dtype).max+1)\n",
    "\n",
    "data_x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97e21cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.frombuffer(b''.join(frames), dtype=np.int16)\n",
    "\n",
    "files = \"./test_audio/sound_wave.wav\"\n",
    "wf = wave.open(files, \"wb\")\n",
    "wf.setnchannels(CHANNELS)\n",
    "wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "wf.setframerate(RATE)\n",
    "wf.writeframes(data)\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd4d76b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio & Image Transforms\n",
    "\n",
    "audio_transforms = ToTensor1D()\n",
    "\n",
    "image_transforms = tv.transforms.Compose([\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Resize(IMAGE_SIZE, interpolation=Image.BICUBIC),\n",
    "    tv.transforms.CenterCrop(IMAGE_SIZE),\n",
    "    tv.transforms.Normalize(IMAGE_MEAN, IMAGE_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5aaa79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio loading\n",
    "\n",
    "paths_to_audio = glob.glob('./demo/audio/*.wav')\n",
    "\n",
    "audio = []\n",
    "for path_to_audio in paths_to_audio:\n",
    "    # track = 1D numpy array of audio wave form, _ = samppling rate \n",
    "    track, _ = librosa.load(path_to_audio, sr=SAMPLE_RATE, dtype=np.float32)\n",
    "\n",
    "    if len(track) < LENGTH:\n",
    "    # Pad with zeros if the tensor is shorter\n",
    "        track = torch.nn.functional.pad(track, (0, LENGTH - len(track)))\n",
    "    elif len(track) > LENGTH:\n",
    "    # Truncate if the tensor is longer\n",
    "        track = track[:LENGTH]\n",
    "\n",
    "    # compute spectrograms using trained audio-head (fbsp-layer of ESResNeXt)\n",
    "    # thus, the actual time-frequency representation will be visualized\n",
    "    spec = aclp.audio.spectrogram(torch.from_numpy(track.reshape(1, 1, -1)))\n",
    "    spec = np.ascontiguousarray(spec.numpy()).view(np.complex64)\n",
    "    pow_spec = 10 * np.log10(np.abs(spec) ** 2 + 1e-18).squeeze()\n",
    "\n",
    "    audio.append((track, pow_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "014356a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image loading\n",
    "paths_to_images = glob.glob('./demo/images/*.jpg')\n",
    "\n",
    "images = list()\n",
    "for path_to_image in paths_to_images:\n",
    "    with open(path_to_image, 'rb') as jpg:\n",
    "        image = simplejpeg.decode_jpeg(jpg.read())\n",
    "        images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbf1059b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\TUK\\Thesis\\Projects\\AudioCLIP\\AudioCLIP-master\\.venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# AudioCLIP handles raw audio on input, so the input shape is [batch x channels x duration]\n",
    "audio = torch.stack([audio_transforms(track.reshape(1, -1)) for track, _ in audio])\n",
    "# standard channel-first shape [batch x channels x height x width]\n",
    "images = torch.stack([image_transforms(image) for image in images])\n",
    "# textual input is processed internally, so no need to transform it beforehand\n",
    "text = [[label] for label in LABELS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60c71e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtaining Embeddings\n",
    "\n",
    "# AudioCLIP's output: Tuple[Tuple[Features, Logits], Loss]\n",
    "# Features = Tuple[AudioFeatures, ImageFeatures, TextFeatures]\n",
    "# Logits = Tuple[AudioImageLogits, AudioTextLogits, ImageTextLogits]\n",
    "\n",
    "((audio_features, _, _), _), _ = aclp(audio=audio)\n",
    "((_, image_features, _), _), _ = aclp(image=images)\n",
    "((_, _, text_features), _), _ = aclp(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9758c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of Embeddings\n",
    "# The AudioCLIP's output is normalized using L2-norm\n",
    "\n",
    "audio_features = audio_features / torch.linalg.norm(audio_features, dim=-1, keepdim=True)\n",
    "image_features = image_features / torch.linalg.norm(image_features, dim=-1, keepdim=True)\n",
    "text_features = text_features / torch.linalg.norm(text_features, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12a89e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtaining Logit Scales\n",
    "#Outputs of the text-, image- and audio-heads are made consistent using dedicated scaling terms for each pair of modalities.\n",
    "#The scaling factors are clamped between 1.0 and 100.0.\n",
    "\n",
    "scale_audio_image = torch.clamp(aclp.logit_scale_ai.exp(), min=1.0, max=100.0)\n",
    "scale_audio_text = torch.clamp(aclp.logit_scale_at.exp(), min=1.0, max=100.0)\n",
    "scale_image_text = torch.clamp(aclp.logit_scale.exp(), min=1.0, max=100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3121148",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computing Similarities\n",
    "# Similarities between different representations of a same concept are computed using [scaled](#Obtaining-Logit-Scales) dot product (cosine similarity).\n",
    "\n",
    "logits_audio_image = scale_audio_image * audio_features @ image_features.T\n",
    "logits_audio_text = scale_audio_text * audio_features @ text_features.T\n",
    "logits_image_text = scale_image_text * image_features @ text_features.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ccc74da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tFilename, Audio\t\t\tTextual Label (Confidence)\n",
      "\n",
      " alarm_clock_3-120526-B-37.wav ->\t\t    alarm clock (97.82%)\n",
      "                 audiofile.wav ->\t\t    mouse click (42.57%)\n",
      "     car_horn_1-24074-A-43.wav ->\t\t    mouse click (36.61%)\n",
      "           cat_3-95694-A-5.wav ->\t\t            cat (99.73%)\n",
      "     coughing_1-58792-A-24.wav ->\t\t       coughing (76.33%)\n",
      "                door_knock.wav ->\t\tkeyboard typing (49.13%)\n",
      "     thunder_3-144891-B-19.wav ->\t\t   thunderstorm (96.92%)\n"
     ]
    }
   ],
   "source": [
    "### AUDIO Classification\n",
    "\n",
    "print('\\t\\tFilename, Audio\\t\\t\\tTextual Label (Confidence)', end='\\n\\n')\n",
    "\n",
    "# calculate model confidence\n",
    "confidence = logits_audio_text.softmax(dim=1)\n",
    "for audio_idx in range(len(paths_to_audio)):\n",
    "    # acquire Top-3 most similar results\n",
    "    conf_values, ids = confidence[audio_idx].topk(1)\n",
    "\n",
    "    # format output strings\n",
    "    query = f'{os.path.basename(paths_to_audio[audio_idx]):>30s} ->\\t\\t'\n",
    "    results = ', '.join([f'{LABELS[i]:>15s} ({v:06.2%})' for v, i in zip(conf_values, ids)])\n",
    "\n",
    "    print(query + results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "40aa644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "config_path = \"./protocols/audioclip-esc50.json\"\n",
    "\n",
    "config = json.load(open(config_path))\n",
    "config = defaultdict(None, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "79cabd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = config['Model']['class']\n",
    "model_args = config['Model']['args']\n",
    "\n",
    "optimizer_class = config['Optimizer']['class']\n",
    "optimizer_args = config['Optimizer']['args']\n",
    "\n",
    "if 'Scheduler' in config:\n",
    "    scheduler_class = config['Scheduler']['class']\n",
    "    scheduler_args = config['Scheduler']['args']\n",
    "else:\n",
    "    scheduler_class = None\n",
    "    scheduler_args = None\n",
    "\n",
    "dataset_class = config['Dataset']['class']\n",
    "dataset_args = config['Dataset']['args']\n",
    "\n",
    "transforms = config['Transforms']\n",
    "performance_metrics = config['Metrics']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "200cf185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model.audioclip.AudioCLIP'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3900d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
